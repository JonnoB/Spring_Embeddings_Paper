---
title: "Untitled"
author: "Jonathan Bourne"
date: "29 April 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---
R spatial analysis
https://www.r-spatial.org/r/2019/09/26/spatial-networks.html

foremost -i /dev/sda3 -t R -o /root/restored/

This script is for performing the spring analysis on IEEE_118. The output of this script is used in the Spring Embeddings project. It was kept as a separate script as the outputs my be used more generally in the PhD.

The script requires that the IEEE_118 network has been generated already


This will chunk need to be copy and pasted into the console or the whole RMD file pasted into a new RMD file in AWS.

~For myriad
you can use qdel $job_id to delete specific jobs/arrays, or qdel '*' to delete all your jobs

  /tmpdir/job/2059520.10/
  
tar -xzvf files_from_job_2063078.3.tgz -C ~/ --strip-components 2

```{r}
# library("RStudioAMI")
# 
# #Follow instructions to link to dropbox
# linkDropbox()
# 
# #excluding from syncing will probably have to be done a few times, if you have many things in your drop box
# excludeSyncDropbox("*")
# 
# 
# #Once everything is excluded get the key dropbox folders for these simulations
# includeSyncDropbox("IEEE_Networks")
# includeSyncDropbox("Flow_Spring_System")
# includeSyncDropbox("Useful_PhD__R_Functions")
```

#Set up

```{r}

packages <- c("rlang", "tidyverse", "igraph", "devtools", "minpack.lm", "foreach", "doParallel" )

new.packages <- packages[!(packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

sapply(packages, library, character.only = TRUE)



#install_github("JonnoB/PowerGridNetworking")
library(PowerGridNetworking)

#Set up file system to read the correct folders this switches between aws and windows mode

#creates the correct root depending on whether this is on the cloud or not
if(dir.exists("/home/jonno")){
  #This folder is for use on my machine
  project_folder <- "/home/jonno/Dropbox/IEEE_Networks"
  basewd <- "/home/jonno"
}else{
  #This is for the folder that is on the cloud
  project_folder <- "~/Dropbox/IEEE_Networks"
  basewd <- "~/Dropbox"
}

IEEE_networks <- file.path(project_folder, "IEEE_network_files") #The folder all the files are stored in for the project
power_grid_graphs_path <- file.path(project_folder, "power_grid_graphs") #The path where the base igraph representations of the power grids are
IEEE_permuted_path <- file.path(power_grid_graphs_path, "IEEE_permuted") #The permuted base IEEE-118 igraphs are stored here
collapse_sets <- file.path(project_folder, "collapse_sets") #the full collapse set of each power grid and the permutations are stored here
collapse_set_summaries <- file.path(project_folder, "collapse_set_summaries")
analysis_parameter_file_path <- file.path(project_folder, "analysis_parameter_files")
HPC_startup_parameter_file_path <- file.path(project_folder, "HPC_parameter_files")

c(IEEE_networks, power_grid_graphs_path, IEEE_permuted_path, collapse_sets, collapse_set_summaries ) %>% walk(~{
  if(!file.exists(.x)) dir.create(.x, recursive = T)
})

#Load some other useful functions
list.files(file.path(basewd, "Useful_PhD__R_Functions"), pattern = ".R", full.names = T) %>%
  walk(~source(.x))

list.files(file.path(basewd, "Flow_Spring_System"), pattern = ".R", full.names = T) %>%
  walk(~source(.x))

#Load the IEEE-118 network
#This network is created by the markdown script Create_IEEE_NEtworks.Rmd also in this repository
IEEE_118 <- readRDS(file = file.path(IEEE_networks, "IEEE_118_igraph.rds"))
```



#PL attack IEEE118
##Edges
```{r}

# 
# setwd(file.path(project_folder, "IEEE118_edges"))
# 
# #rep <-5 #I don't think this does anything. If nothing breaks then delete.
# alpha_vector <- c(1, 1.02, 1.01, 1.005, 1.05, 1.1, 1.2, 1.5, 2, 3, 5, 7, 10, 15, 20, 30, 50, 100, 200, Inf)
# alpha_vector
# set.seed(21256)
# DeleteOrders_Edges <- MultiAttackOrder(IEEE_118, Target ="Edges", Sims = 100, Name = "Link")
# 
# #Create the simulations for each of the alpha avalues using 100 simulations for each
# alpha_vector %>% walk(~{
#   gProp <- Proportional_Load(IEEE_118, alpha = .x)
# 
#   folder <- paste0("alpha_value_",  .x*100)
#   #create folder if it doesn't already exist
#   if(!file.exists(folder)){
#     dir.create(folder)
#   }
# 
#   CascadeMode <- ifelse(is.finite(.x), TRUE, FALSE)
# 
#   SaveMultiAttacks(gProp, DeleteOrders_Edges, folder,
#                    TotalAttackRounds = 1000,
#                    CascadeMode = CascadeMode,
#                    Demand = "Load_MW",
#                    Generation = "Generation_MW",
#                    EdgeName = "Link",
#                    VertexName = "name",
#                    Net_generation = "Net_Generation",
#                    Target = "Edges")
# 
# }
# 
# )
# 


```

##Nodes
I am not doing nodes for this experiment. Previous results (now deleted) suggest that node analysis is not hugely different anyway

#Target orders

Create the target orders for the scrambled edge values
##ec values to attack
```{r}
#The alpha/ec values to scramble
Scramble_ec_values <- c(1.005, 1.025, 1.05, 1.1, 1.2, 1.5, 2, 3, 5, 7, 10, 20) 

#The fraction of edges that will be scrmabled for each scrambled alpha
fract_vect <- c(1, 0.75, 0.5, 0.25) #0.75 can also be added but may overlap with the others too much

```


Generate the target orders that will be used to ensure that the random edge permutations are consistant across the analysis.

This can take a while for large numbers (e.g 10k samples), as each graph needs to be generated and the new alpha value found.
```{r}
# 
# target_orders <- fract_vect %>% map_df(~{
# 
#   file_name <- paste0("target_orders_fract_", .x*100, ".rds")
# 
#   current_fract <- .x
# 
#   if(!file.exists(file.path(project_folder, "target_orders", file_name))){
# 
#   set.seed(123)
#   random_seeds <- sample(1:100000, 10000)
# 
#   target_orders <- Scramble_ec_values %>% map_df(~{
# 
#     print(.x)
#     #create network
#     Scrambled_edge_cap <-  Proportional_Load(IEEE_118, alpha = .x)
#     #permute edges
#     print("creating the random permutations")
#     seed_alpha <- Permute_excess_capacity(Scrambled_edge_cap, random_seeds, fract = current_fract)
#     #take subselection
#     target_orders <- sub_selection_of_seed_alpha(seed_alpha, total_samples = 10, seed = 123) %>%
#       mutate(ec = .x,
#              v = 1:n(),
#              scramble_fract = current_fract)
# 
#     return(target_orders)
# 
#   })
# 
#   saveRDS(target_orders, file.path(project_folder, "target_orders", file_name))
# 
# } else {
# 
#   target_orders  <- readRDS(file.path(project_folder, "target_orders", file_name))
# 
# }
#  return(target_orders)
# 
# })

```

#Fixed EC multi-attack

Two loops

* select fraction
* select scramble value
* attack

The attack goes through all the fractions of scramble as well as all the combinations of random scramble.
It may take a couple of days to calculate. Best done on the cloud if possible

##Edge attack
```{r}
# IEEE_118 <- readRDS(file = file.path(IEEE_networks, "IEEE_118_igraph.rds"))
# 
# 
# #These two lines of code are in the PL attack section. the seed ensures the attack order is the same, although it probably doesn't have a very large effect
# set.seed(21256)
# DeleteOrders_Edges <- MultiAttackOrder(IEEE_118, Target ="Edges", Sims = 100, Name = "Link")
# 
# #fract_vect %>%
# fract_vect[1] %>%   #used for splitting up the attacks across the fractions
#   walk(~{
# 
#   current_fract <- .x
# 
# Scramble_ec_values %>%
#   walk(~{
# 
#     target_orders_x <- target_orders %>%
#       filter(ec == .x, scramble_fract == current_fract)
# 
#     folder <- paste0("alpha_value_",  .x*100) %>%
#       file.path(project_folder, paste0("IEEE_permute_edge_ec_Edge_fract_", current_fract), .)
# 
#     #create folder if it doesn't already exist
#     if(!file.exists(folder)){
#       dir.create(folder, recursive = TRUE) #create all folders in the path if necessary
#     }
# 
#     #
#     #attack network using the deletion orders
#     #
# 
#     setwd(folder)
#     #create network
#     Scrambled_edge_cap <-  Proportional_Load(IEEE_118, alpha = .x)
#     #attack!
#     Scrambled_Edge_SaveMultiAttacks(target_orders_x, Scrambled_edge_cap,
#                                     "constant_ec_v", DeleteOrders_Edges, Target = "Edges", fract = current_fract)
# 
#   })
# 
# })
```


#Calculate strain of the selected networks


##Common values
The below chunk was put in due to simulations being run with different parameters resulting in out of synch heights. The common values will be used for both the proportionally loaded edges and the ec scrambled edges
```{r}
common_time <- 0.01
common_Iter <- 20000
common_tol <- 1e-10
common_mass <- 1


Standardisesd_solution_finder <- function(g, common_time, common_Iter, common_tol, common_mass){
  #This function is just a wrapper to tidy up the code chucnks in finding various strain heights for IEEE_118

   current_graph  <- g %>%
    set.edge.attribute(. , "distance", value = 1) %>%
    Calc_Spring_Youngs_Modulus(., "PowerFlow", "Link.Limit", minimum_value = 100, stretch_range = 1000) %>%
    set.edge.attribute(., "Area", value = 1) %>%
  Normalize_load(., EdgeName = Link, VertexName = name, Net_Generation = Net_Generation, capacity = Link.Limit)


  List_of_BiConComps <- Create_balanced_blocks(current_graph, force = "Net_Generation")

    giant_componant <-List_of_BiConComps %>% map_dbl(~vcount(.x)) %>% which.max()

  #use the largest block to set the simulation parameters k and m.
  #k needs to be sufficiently stretch to allow enough topology variation. otherwise all that happens is a surface angled in the direct of net power flow. Which is interesting but not that interesting
      OriginBlock_complete <- Find_network_balance(g = List_of_BiConComps[[giant_componant]],
                                                   force ="Net_Generation",
                                                   flow = "PowerFlow",
                                                   distance = "distance",
                                                   capacity = "Link.Limit",
                                                   tstep = common_time,
                                                   tol = common_tol,
                                                   maxIter = common_Iter,
                                                   mass = common_mass,
                                                   verbose = FALSE)

      final_z <- Create_stabilised_blocks(g = current_graph,
                                          OriginBlock = OriginBlock_complete,
                                          OriginBlock_number = giant_componant,
                                          force ="Net_Generation",
                                          flow = "PowerFlow",
                                          distance = "distance",
                                          capacity = "Link.Limit",
                                          tstep = common_time,
                                          tol = common_tol,
                                          maxIter = common_Iter,
                                          mass = common_mass,
                                          verbose = FALSE)


}

```


##Proportionally loaded spring system

calcualtes the embeddings of the proportionally loaded systems indicated in the alpha vector

```{r}

# IEEE_118 <- readRDS(file.path(project_folder,"IEEE_network_files", "IEEE_118_igraph.rds"))
# 
# 
# #Network loadings to find the angles for
# alpha_vector <- c(1, 1.02, 1.01, 1.005, 1.05, 1.1, 1.2, 1.5, 2, 2.5, 3, 5, 7, 10, 15, 20, 50, 100, 200, Inf)
# 
# if(!file.exists(file.path(project_folder, "Solved_height_networks_PL"))){
#   dir.create(file.path(project_folder, "Solved_height_networks_PL"))
# }
# 
# 
# #calculate theta for all values of alpha
# alpha_vector %>% walk(~{
# 
#   alpha <- .x
#   file_path <- file.path(project_folder,"Solved_height_networks_PL", paste0("IEEE_118_alpha_", alpha, ".rds"))
#   
#   if(file.exists(file_path)){
#     
#     print("file exists continueing to next file")
#     
#   }else {
#     
#       print(paste("alpha value", alpha))
# 
#   current_graph <- IEEE_118 %>% Proportional_Load(., alpha = alpha)
# 
#   final_z <- Standardisesd_solution_finder(current_graph, common_time, common_Iter, common_tol, common_mass)
# 
#   write_rds(final_z, file_path)
#     
#   }
# 
# })
# 


```


##Scrambled edge embeddings

Calculates the embeddings of the scrambled edge systems across all the fractions

This code is designed to be spread across several machines. It doesn't seem to paralellise well, I am doing something wrong but haven't worked out how to fix it, hence the multiple-machines solution.

An alternative is to assign 1 fraction per machine. Do whatever you want.

```{r}
# 
# cpu_id <- 1
# number_of_cpus <- 4
# 
# target_orders2 <- target_orders %>%
#   mutate(file_path = file.path(project_folder,
#                                paste0("constant_ec_from_alpha_fract_", scramble_fract),
#                                paste0("Solved_height_networks_alpha_", ec*100),
#                                paste0("IEEE_118_alpha_", v, ".rds")),
#          calc_on_this_machine = rep_along(file_path, 1:number_of_cpus)) %>%
#   filter(calc_on_this_machine == cpu_id,
#          ec <= 20)
# 
# 1:nrow(target_orders2) %>%
#   walk(~{
# 
#     target <- target_orders2 %>%
#       slice(.x)
# 
#     #does the file exist?
#     if(file.exists(target$file_path)){
#       print("file exists continueing to next file")
#     }else {
# 
#       #if the file doesn't exist ensure that the folder is created and then calculate
#       if(!dir.exists(dirname(target$file_path))){
#         dir.create(dirname(target$file_path), recursive = TRUE)
#       }
# 
#       print(paste0("fraction ", target$scramble_fract ," ec value ", target$ec, ". v ",target$v))
#       #create network
#       Scrambled_edge_cap <-  Proportional_Load(IEEE_118, alpha = target$ec)
# 
#       #create dataframe of new edge limits
#       temp <-Create_scrambled_edges(Scrambled_edge_cap, target$seed, fract = target$scramble_fract)
# 
#       print(mean(temp$alpha))
# 
#       final_z <- Standardisesd_solution_finder(Scrambled_edge_cap %>%
#                                                  set.edge.attribute(., "Link.Limit", value = temp$Link.Limit),
#                                                common_time, common_Iter, common_tol, common_mass)
# 
#       write_rds(final_z, target$file_path)
# 
#     } #proceed to next iteration
# 
#   })

```



#Permuted IEEE118

This chunk creates 30 permutates of IEEE where the generation nodes have been permuted and the demand nodes have been permuted.

We then repeat the previous set of scrambles for each permutation

##Create networks
```{r}
#The path the target orders will be stored at
Permuted_IEEE_118_path <- file.path(project_folder, "target_orders", "Permuted_IEEE_118")

#create the folder if necessary
if(!dir.exists(Permuted_IEEE_118_path)){
  dir.create(Permuted_IEEE_118_path, recursive = TRUE)
}


#Create a list of IEEE-118 networks where the demand and gen node values are internally permuted
set.seed(1235)
random_seeds <- sample(1:10000, 30)

Permuted_IEEE_118_list <- random_seeds %>% map(~{
  g <- Permute_IEEE(IEEE_118, .x)
  
  g <-  BalencedGenDem(g, 
                        Demand = "Load_MW",
                        Generation = "Generation_MW",
                        OutputVar = "Net_Generation")
  
  SlackRef <- SlackRefFunc(g, name = "name", Generation = "Generation_MW")
  
  g <- PowerFlow(g, SlackRef$name, EdgeName ="Link", VertexName = "name", Net_generation = "Net_Generation")
})

#Save the networks so that they can be used by the HPC 

1:length(Permuted_IEEE_118_list) %>%
  walk(~{
    saveRDS(Permuted_IEEE_118_list[[.x]], 
            file.path(IEEE_permuted_path, 
                      paste0("Permutation_", .x, ".rds")))
  })

```


##target orders

Generate the target orders in parallel... becuase ain't nobody got time for that
```{r}

registerDoParallel(cores=6) #RUNS ON ALL CORES!
foreach(n = 1:length(Permuted_IEEE_118_list),
        .packages = c("PowerGridNetworking", "rlang", "dplyr", "igraph", "stringr", "purrr", "tidyr")) %dopar%{

  file_name <- paste0("target_permutation_", n, ".rds")
  #only creat the target orders if necessary
  if(file.exists(file.path(Permuted_IEEE_118_path, file_name))){

    print("file exists continueing to next network permutation")
  } else{

    target_orders_temp <- Create_target_orders_for_strain_test(Permuted_IEEE_118_list[[n]], fract_vect, Scramble_ec_values,
                                                               total_sample_space = 10000, #Larger number mean bigger extremes but it takes much longer. I choose 10k as a painful slow medium
                                                               required_samples_out = 10,
                                                               seed = n ) #previously a constant 123

    saveRDS(target_orders_temp, file.path(Permuted_IEEE_118_path, file_name))
  }

}
  stopImplicitCluster()


  
test <-   target_orders_temp %>%
    group_by(seed) %>%
    summarise(counts = n())

```


##Attack ec

Here I attack the randomly permuted edges where the EC is fixed. The concept with the below chunk is that each of the machines used to get the attack results works on each permutation of IEEE-118. This means that each attack set is finished before teh next one is started. This allows me to check that everything is working the way it should.

Outer loop: Permutation to be calculated
Inner loop: the particular target order

```{r}

cpu_id <- 1
number_of_cpus <- 1

set.seed(21256)
DeleteOrders_Edges <- MultiAttackOrder(IEEE_118, Target ="Edges", Sims = 100, Name = "Link")

1:30 %>%
  walk(~{ #Outer loop choose the permutation

    #Select the current graph permutation to work on
    current_graph <- Permuted_IEEE_118_list[[.x]]

    #load the graph scramble seeds for this network permutation.
    target_orders <- readRDS(file.path(Permuted_IEEE_118_path,  paste0("target_permutation_", .x, ".rds"))) %>%
      mutate(folder_path = file.path(project_folder,
                                     "Permuted_IEEE_118_collapse_set",
                                     paste0("Permutation_", .x),
                                     paste0("fract_", fract, "_ec_", ec*100, "_v_", v)))     %>%
      arrange(-ec) %>%#order the clculations so that the slow ones are first. this makes the paralellisation more efficient
      #by preventing cpu's finishing whilst slow calculations are in progress. This way the the cpu's will finish each permutation on the smallest fastest ec values, minimising non-active cpu time
      mutate(calc_on_this_machine = rep_along(folder_path, 1:number_of_cpus),
             ID  = 1:n()) %>%

      filter(calc_on_this_machine == cpu_id) #keep only the elements to be calculated on this machine

    Scrambled_Edge_SaveMultiAttacks_parallel(current_graph, target_orders, DeleteOrders_Edges , Target = "Edges", cores = 5)
  })

#You can't make a single dataframe to do the multi attack until you can work out how to load different networks according to which permutation is being used

#Creates a dataframe of all the permutations all 30 sets.
#This reduces inactive cpus at the end of each set
#It Also makes the code easier to read
# target_orders_attack <- 1:30 %>%
#   map_df(~{ #Outer loop choose the permutation
#
#     #Select the current graph permutation to work on
#     current_graph <- Permuted_IEEE_118_list[[.x]]
#
#     #load the graph scramble seeds for this network permutation.
#     target_orders <- readRDS(file.path(Permuted_IEEE_118_path,  paste0("target_permutation_", .x, ".rds"))) %>%
#       mutate(folder_path = file.path(project_folder,
#                                      "Permuted_IEEE_118_collapse_set",
#                                      paste0("Permutation_", .x),
#                                      paste0("fract_", fract, "_ec_", ec*100, "_v_", v)),
#              calc_on_this_machine = rep_along(folder_path, 1:number_of_cpus),
#              ID  = 1:n()) %>%
#       filter(calc_on_this_machine == cpu_id) #keep only the elements to be calculated on this machine
#
#   })
#
#    Scrambled_Edge_SaveMultiAttacks_parallel(current_graph, target_orders_attack, DeleteOrders_Edges , Target = "Edges", cores = detectCores())

```

##Strain Calc

```{r}


#library(future.apply)
cpu_id <- 1
number_of_cpus <- 1

permuted_strain_folder <- file.path(project_folder,"Permuted_IEEE_118_strain_set")

if(!file.exists(permuted_strain_folder)){
  dir.create(permuted_strain_folder)
}

#Create the target orders across all permutations
#ensures the file path links to the strain folder not the collapse folder
target_orders_strain <- 1:length(Permuted_IEEE_118_list) %>% map_df(~{
  
  current_graph <- Permuted_IEEE_118_list[[.x]]
  
  file_name <- paste0("target_permutation_", .x, ".rds")
  #only creat the target orders if necessary
  
  target_orders2 <-read_rds(file.path(Permuted_IEEE_118_path, file_name)) %>%
    mutate(file_path = file.path(permuted_strain_folder,
                                 paste0("Permutation_", .x),
                                 paste0("fract_", fract, "_ec_", ec*100, "_v_", v, ".rds")),
           permute = .x,
           calc_on_this_machine = rep_along(file_path, 1:number_of_cpus),
           ID  = 1:n()) %>%
    filter(calc_on_this_machine == cpu_id)
}
)


for(X in 1:nrow(target_orders_strain)){ #use future lapply to parallelize the process
  
  target <- target_orders_strain %>%
    slice(X)
  
  #does the file exist?
  if(file.exists(target$file_path)){
    print(paste0("File ", basename(target$file_path)," for permute ", target$permute, " exists continueing to file: ", X+1, " of ", nrow(target_orders_strain)))
  }else {
    
    #if the file doesn't exist ensure that the folder is created and then calculate
    if(!dir.exists(dirname(target$file_path))){
      dir.create(dirname(target$file_path), recursive = TRUE)
    }
    
    print(paste0("fraction ", target$fract ," ec value ", target$ec, ". v ",target$v))
    #create network
    Scrambled_edge_cap <-  Proportional_Load(Permuted_IEEE_118_list[[target$permute]], alpha = target$ec)
    
    #create dataframe of new edge limits
    temp <- Create_scrambled_edges(g = Scrambled_edge_cap, target$seed, fract = target$fract)
    
    final_z <- Standardisesd_solution_finder(Scrambled_edge_cap %>%
                                               set.edge.attribute(., "Link.Limit", value = temp$Link.Limit),
                                             common_time, common_Iter, common_tol, common_mass)
    
    
    write_rds(final_z, target$file_path)
      print(paste("Convergence", X, "of", nrow(target_orders_strain) ,"complete proceeding to next calculation") )
    
  }
}

```

##parallel version 

doesn't work very well
```{r}

#foreach version
#This doesn't work on my machine but does work on the cloud. I don't know why could be to do with the blas/lapak libraries
registerDoParallel(cores=4)
foreach(X = 1:nrow(target_orders_strain), 
        .packages = c("PowerGridNetworking", "rlang", "dplyr", "igraph", "stringr", "purrr", "tidyr")) %do% { #use future lapply to parallelize the process
          
          target <- target_orders_strain %>%
            slice(X)
          
          #does the file exist?
          if(file.exists(target$file_path)){
            print("file exists continueing to next file")
          }else {
            
            #if the file doesn't exist ensure that the folder is created and then calculate
            if(!dir.exists(dirname(target$file_path))){
              dir.create(dirname(target$file_path), recursive = TRUE)
            }
            
            print(paste0("fraction ", target$fract ," ec value ", target$ec, ". v ",target$v))
            #create network
            Scrambled_edge_cap <-  Proportional_Load(Permuted_IEEE_118_list[[target$permute]], alpha = target$ec)
            
            #create dataframe of new edge limits
            temp <- Create_scrambled_edges(g = Scrambled_edge_cap, target$seed, fract = target$fract)
            
            print(mean(temp$alpha))
            
            final_z <- Standardisesd_solution_finder(Scrambled_edge_cap %>%
                                                       set.edge.attribute(., "Link.Limit", value = temp$Link.Limit),
                                                     common_time, common_Iter, common_tol, common_mass)
            
            
            write_rds(final_z, target$file_path)
            print("Convergence", X, "of", nrow(target_orders_strain) ,"complete proceeding to next calculation", )
            
          }
        }
stopImplicitCluster()

```

##Extract attacks
```{r}

permutation <- 2

Attack_folder <- paste0("/home/jonno/Dropbox/IEEE_Networks/Permuted_IEEE_118_collapse_set/Permutation_", permutation)

Summary_folder <- file.path(project_folder, "Summary_Permuted_IEEE_118_collapse_set",
                                paste0("Permutation_",  permutation)) ###########change this! to make multiple
    
    #Create the summary folder for that fraction
    if(!file.exists(Summary_folder)){
      dir.create(Summary_folder, recursive = T)
    }
        #extract everything from each of the sub folders of each fraction
ExtractAttackStats_parallel(RootFolder = Attack_folder, 
                            file.path(Summary_folder),
                            Generation = "Net_Generation",
                            EdgeName = "Link",
                            cores = 7)


        ExtractAttackStats(RootFolder = Attack_folder, 
                           file.path(Summary_folder),
                           Generation = "Net_Generation",
                           EdgeName = "Link")


```

##plot results of single permutation

This result shows that permuting the nodes to change the profile still produces more accurate results
```{r}


test <- Create_strain_alpha_results_df(g = Permuted_IEEE_118_list[[2]], 
                                       target_orders = target_orders_strain %>%
                                         filter(permute==2), 
                                       Summary_folder)

current_permutation <- Permuted_IEEE_118_list[[2]] %>%
  set.edge.attribute(., "distance", value = 1)%>%
  set.edge.attribute(., "Link.Limit", value = Inf)

target_orders_temp <- target_orders_strain %>%
  filter(permute == 2) #%>%
  slice(1:428)
  filter(file.exists(target_orders_strain$file_path),permute==1)

#load strain
Permuted_strain_set_df <- 1:nrow(target_orders_temp) %>%
  map_df(~{
    
    load_file <- target_orders_strain %>% slice(.x)
   print(.x)
    Out <- read_rds(load_file$file_path) %>%
    Calc_line_strain(current_permutation, ., distance = "distance", capacity = "Link.Limit", flow = "PowerFlow") %>%
      summarise(strain = mean(strain)) %>%
      bind_cols(load_file, .)
    
    return(Out)
  }) %>%
  select(-file_path)

Permuted_IEEE_118_results <- list.files(path = Summary_folder, 
                                pattern = ".rds", 
                                full.names = TRUE, 
                                recursive = TRUE) %>%
    map_df(~read_rds(.x)%>%
             mutate(file_path = .x))   %>%
    arrange(-TotalNodes) %>%
    mutate(has_gc = mean_degree_sqrd > 2*mean_degree) %>%
    filter(!has_gc) %>%
    group_by(simulationID, file_path) %>%
    summarise_all(first)  %>%
    select(-file_path) %>%
  rename(file_path = alpha) %>%
  mutate(file_path = basename(file_path)) %>%
  left_join(target_orders_strain %>% #add in target orders to get the simulation params
  mutate(file_path = basename(file_path) %>% gsub(".rds", "", .)) %>% filter(permute == 1)) %>% 
  ungroup %>%
  group_by(ec, v, fract, permute) %>%
  summarise(NodesAttacked = mean(NodesAttacked)) %>%
  left_join(target_orders_strain , by = c("ec", "v", "fract", "permute")) %>% ##join the target orders to get the alpha values
  #The second join may be able to be joined in the first step, but it isn't super important.
  select(ec:NodesAttacked) %>%
  left_join(Permuted_strain_set_df,by = c("ec", "v", "fract", "permute")) %>%
  select(-calc_on_this_machine, -ID) %>%
  ungroup %>%
  mutate(alpha = 1/alpha)


#plot

test  %>%
  filter(complete.cases(.)) %>%
    mutate(alpha = alpha/max(alpha),
           strain = strain/max(strain)) %>%
  gather(., key = type, value = value, alpha:strain) %>%
  ggplot(., aes(y = value, x = NodesAttacked)) + 
  geom_point(aes(colour = as.factor(ec), shape = factor(fract))) +
  geom_smooth( se = FALSE, colour = "black", linetype = 2) +
  facet_wrap(~type, labeller = label_parsed) + #parsing shows the plotmath
  labs(title = "Analysing the spread of outcomes for mean loading and relative strain", 
       x = "Fraction of Edges Attacked", y = "metric value",
       colour = "Original\nsystem\ntolerance")  

```

#do the loess
```{r}
#this is loadded seperately to reduce overhead on the large calculations. Really this analysis should be entirely seperate from the rest of the script.
library(yardstick)

loess_alpha <- loess(formula = NodesAttacked~ alpha, 
      data = Permuted_IEEE_118_results)


loess_strain <- loess(formula = NodesAttacked~ strain, 
      data = Permuted_IEEE_118_results)


model_comp <- Permuted_IEEE_118_results %>%
  mutate(alpha_preds = predict(loess_alpha),
         strain_preds = predict(loess_strain))

metrics(Permuted_IEEE_118_results, NodesAttacked, alpha)

metrics(Permuted_IEEE_118_results, NodesAttacked, strain)


metrics(model_comp, NodesAttacked, alpha_preds)

metrics(model_comp, NodesAttacked, strain_preds)

```


#Dummy Chunk
run all the chunks above
```{r}

```


#Strain and quantity

I beleive that strains link to robustness is mediated throught the concentration of demand and generation nodes.
I will test this by doing the following on the largest block of the IEEE 118 network
Using the largest component prevents dead areas of the network and ensures that the all the experiments are topologically identical

5 quantities of generator fraction is used either 17, 12, 8, 4, 1, or a single generator 
20 random samples of each generator level
5 alpha levels 1, 1.5, 2, 5, Inf

This makes 500 different combinations to try + 5 at 100% of all generators

I then 

```{r}

IEEE_118 <- readRDS(file = file.path(IEEE_networks, "IEEE_118_igraph.rds"))

#alpha levels of the concentrator
alpha_conc <- c(1, 1.5, 2, 5, Inf)

List_of_BiConComps <- Create_balanced_blocks(IEEE_118, force = "Net_Generation")

giant_componant <-List_of_BiConComps %>% map_dbl(~vcount(.x)) %>% which.max()

exp_IEEE <- List_of_BiConComps[[giant_componant]]

#In this case the base slack ref will be the largest power consumer
SlackRef_conc <- "59"

#Generate Attack orders
set.seed(21256)
DeleteOrders_Concentrator_Edges <- MultiAttackOrder(exp_IEEE, Target ="Edges", Sims = 100, Name = "Link")  


#get the node id of all the generators
gen_id <- as_data_frame(exp_IEEE, what = "vertices") %>% filter(Generation_MW >0) %>% pull(name) 

#make a df of all combinations of sample size and sample iteration
combs <- expand.grid(x = c(4,8,12), y = 1:20) %>%
  as_tibble 

quant_alpha_comb <- expand.grid(quant = c(17, 12, 8, 4, 1), alpha = alpha_conc) %>%
  as_tibble


active_gen_df <- map2_df(.x  = combs$x, .y = combs$y, ~{
  
  tibble(active_gen_id =  sample(gen_id, .x, replace = FALSE), sample = .y, quant = .x)
  
}) %>%
  bind_rows(tibble(active_gen_id =  gen_id, sample = 1, quant = 17)) %>% #there is only 1 combination of all generators
  bind_rows(tibble(active_gen_id =  gen_id, sample = 1:17, quant = 1)) %>% #there are only 17 for sample size of 1 so this is added on after the others
left_join(quant_alpha_comb,., by = "quant" ) #join so that each specific quant-sample combo is represented at each alpha quant

#the unique simulation combos to be calculated
simulation <-active_gen_df %>% 
  select(-active_gen_id) %>%
  distinct()

cpu_id <- 4
number_of_cpus <- 4
cpu_vector <- rep(1:number_of_cpus, length.out = nrow(simulation))
cpu_sims <-(1:nrow(simulation))[cpu_vector==cpu_id]


cpu_sims %>% walk(~{
  #set the simulation to calculate
  current_sim <- simulation %>%
    slice(.x)
  
  #folder name is 
  folder <- file.path(project_folder, 
                      "Concentrator",
                      paste0("Concentrator_quant_",
                             current_sim$quant, 
                             "_alpha_", 
                             current_sim$alpha, 
                             "_sample_", 
                             current_sim$sample) )
  #create folder if it doesn't already exist
  if(!file.exists(folder)){
    dir.create(folder)
  }
  
  #active gen id's for that simulation
  active_gen_id <-active_gen_df %>%
    filter(sample == current_sim$sample, quant == current_sim$quant, alpha == current_sim$alpha) %>%
    pull(active_gen_id)
  
  #set active generators
  current_gen <- as_data_frame(exp_IEEE, what = "vertices") %>%
    mutate(Generation_MW = if_else(name %in% active_gen_id, Generation_MW, 0),
           Perc_Gen = Generation_MW/sum(Generation_MW), #find the percentage of total gen for each generator
           Generation_MW = Perc_Gen*sum(Load_MW), #scale generation to match the demand
           Net_Generation = Generation_MW-Load_MW) #reset the net generation column
  
  #set alpha level
  current_g <-as_data_frame(exp_IEEE) %>%
    graph_from_data_frame(., directed = FALSE, vertices = current_gen) %>%
    PowerFlow(., SlackRef_conc, Net_generation = "Net_Generation") %>%
    Proportional_Load(., alpha = 1)
  
  
  #try to speed up simulation by setting not using cascade mode when alpha is Inf
  CascadeMode <- ifelse(is.finite(current_sim$alpha), TRUE, FALSE)
  
  #attack grid
  
  SaveMultiAttacks(current_g, 
                   DeleteOrders_Concentrator_Edges, 
                   folder, 
                   TotalAttackRounds = 1000, 
                   CascadeMode = CascadeMode,
                   Demand = "Load_MW",
                   Generation = "Generation_MW",
                   EdgeName = "Link", 
                   VertexName = "name", 
                   Net_generation = "Net_Generation",
                   Target = "Edges")
  
})
  
```


#extract the data
```{r}


ExtractAttackStats(RootFolder = file.path(project_folder, "Concentrator"), 
                   NewfolderPath = file.path(project_folder, "Concentrator_Summary"), 
                   Generation = "Net_Generation",
                   EdgeName = "Link",
                   PowerFlow = "PowerFlow",
                   Link.Limit = "Link.Limit")



#Load the saved files
AttackRoundData <- list.files(path =file.path(project_folder, "Concentrator_Summary"), 
                              pattern = ".rds", 
                              full.names = TRUE)  %>%
   map_df(~read_rds(.x)) 


test <- AttackRoundData %>%
  arrange(-TotalNodes) %>%
  mutate(has_gc = mean_degree_sqrd > 2*mean_degree) %>%
  filter(!has_gc) %>% 
  group_by(simulationID, alpha) %>%
  summarise_all(first) %>%
  separate(alpha, c("drop1", "drop2","quant","drop3", "alpha_value", "drop4", "sample"), sep = "_") %>%
  mutate(quant = as.integer(quant),
         alpha_value = as.numeric(alpha_value),
         totals = n()) %>%
  select(-contains("drop")) %>%
  group_by(quant, alpha_value) %>%
  summarise_all(mean)

```


#Strain
This looks at how strain functions as a robustness metric

```{r}

Calc_line_strain <- function(g, solved_height_df, distance){
  
  line_strain <-as_data_frame(g) %>% as_tibble %>%
  left_join(., solved_height_df %>% select(node, z), by = c("from"= "node")) %>%
  left_join(., solved_height_df %>% select(node, z), by = c("to"= "node")) %>%
  mutate(dz = abs(z.x-z.y),
         mean_z = (z.x+z.y)/2,
         H = sqrt(dz^2 +{{distance}}^2),
         strain = (H-{{distance}})/{{distance}},
         alpha = Link.Limit/abs(PowerFlow),
         line_load = abs(PowerFlow)/Link.Limit,
         percentile_strain = percent_rank(strain)) %>%
  select(Link, alpha, line_load, dz, H, strain, percentile_strain, mean_z, PowerFlow)
  
}


test <- list.files("/media/jonno/Seagate Expansion Drive/IEEE_Networks/Solved_height_networks", full.names = T) %>%
  map_df(~{
    
    alpha <- basename(.x) %>% gsub("IEEE_118_alpha_", "", .) %>% gsub(".rds", "", .) %>% as.numeric()

    IEEE_118_test<- Proportional_Load(IEEE_118, alpha = alpha) %>%
      set.edge.attribute(. , "distance", value = 1)
    
    read_rds(.x) %>%
      Calc_line_strain(IEEE_118_test , ., distance = distance)

    
  })




test %>%
  ggplot(aes(x = strain, colour = as.factor(alpha))) + geom_density()

test_strain <- test %>%
  mutate(alpha = round(alpha, 5)) %>%
  group_by(alpha) %>%
  summarise(mean = mean(strain),
            median = median(strain),
            weighted.mean = weighted.mean(strain, abs(PowerFlow)),
            counts = n())

test_strain %>%
  ggplot()

test_strain %>%
  gather(key = type, value = strain, -alpha, -counts) %>%
  ggplot(aes(x = 1/alpha, y = strain, colour = type )) + geom_point()

```


#explore convergence

It seems that the algorithm is not converging properly.
I am getting large difference between the infinite model and the a = 10 random ex model.
This below chunk tries to test if 16k and t = 0.3 is good enough, or just what is going on

The algorithm is converging properly. The vairability in the number of nodes needed for complete collapse is so high for low values of alpha that even 100 simualtion has a few dodgy values
```{r}

#calculate theta for all values of alpha
finnesse_prop <-c(1.1, 1.05) %>% map_df(~{
  
  alpha <- .x
  
  print(paste("alpha value", alpha))
  
  current_graph  <- IEEE_118 %>%
    Proportional_Load(., alpha = alpha) %>% 
    set.edge.attribute(. , "distance", value = 1) %>%
    Calc_Spring_Youngs_Modulus(., "PowerFlow", "Link.Limit", 100, 10) %>%
    set.edge.attribute(., "Area", value = 1)
  
  List_of_BiConComps <- Create_balanced_blocks(current_graph, force = "Net_Generation")
  
  #use the largest block to set the simulation parameters k and m.
  #k needs to be sufficiently stretch to allow enough topology variation. otherwise all that happens is a surface angled in the direct of net power flow. Which is interesting but not that interesting
  OriginBlock <- Find_network_balance(List_of_BiConComps[[11]], force = "Net_Generation", 
                                      tstep = 0.01, tol = common_tol, distance = "distance", 
                                      maxIter = 75000, mass = common_mass)
  
  final_z <- Create_stabilised_blocks(current_graph, OriginBlock, 11, force = "Net_Generation", 
                                      tstep = 0.01, tol = common_tol, distance = "distance", 
                                      maxIter = 75000, mass = common_mass) %>%
    mutate(ec = .x)
  
return(final_z)
})


test <-c(1.05, 1.1) %>% map_df(~{
  
test_g <- IEEE_118  %>%
    Proportional_Load(., alpha = .x) %>% 
    set.edge.attribute(. , "distance", value = 1) %>%
    Calc_Spring_Youngs_Modulus(., "PowerFlow", "Link.Limit", 100, 10)
  
  finnesse_prop %>%
    filter(ec == .x) %>%
  Calc_line_strain(test_g, ., distance) 
}) %>%
  mutate(alpha = round(alpha, 5)) %>%
  group_by(alpha) %>%
  summarise(strain = mean(strain))

test2 <- theta_crit_thresh %>%
  mutate(alpha = 1/alpha) %>%
  select(alpha,  NodesAttacked, theta_degs, orig_strain)

```


#Get time to create files

Time based on the PL system suggest That 
0.5 minute for 14
1 minute 30
2 minute 57
7 minutes 118
9 minutes 300

I looked at 118 after running for enough time to create 100 sims it used 2700Mb of ram 
To the set up requirements would be

7*100 = 700 minutes say 12 hours for a 20 minute buffer

Per HPC job times in hours

14  1.5 
30  2
57  5
118 12
300 16

4GB ram just to be on the safe side

These times were eyeballed from a single round of calculation

strain calc
14 about 2 mins say 3 1gb ram
30 between 3.7 and 4.5 say 5
57
118 10 mins per collapse try 1gb of ram
300


```{r}

target_path <- "/home/jonno/Dropbox/IEEE_Networks/collapse_set_summaries"

all_dirs <-list.dirs(target_path) %>% #remove base dir
tibble(value = .) %>%
  filter(grepl("fract_0", value)) %>% pull(value)

completion_time <- all_dirs %>% map_df(~{

  all_files <- .x %>%
  list.files(., full.names = T, recursive = T, pattern = ".rds") 
  
  alpha <- all_files %>% dirname() %>% basename() %>% str_split(., "_", simplify = T) %>% as_tibble() %>%
  pull(V4) %>% as.numeric()
  
 Out <-  all_files %>% file.info(.) %>% as_tibble() %>%
    mutate(graph = all_files %>% dirname %>% dirname %>% basename() %>% gsub("_igraph", "", .) 
           %>% gsub("Permutation_", "", .),
           sim_id = str_extract(basename(all_files), "\\d+") %>% as.integer(),
           alpha = alpha) %>%
  arrange(ctime) %>%
   group_by(alpha) %>%
  mutate(lag = lag(ctime),
    mins = difftime(ctime, lag, units = "mins" ),
    hours = difftime(ctime, lag, units = "hours" )) %>% 
   select(graph, alpha, mins, size, hours, ctime, lag, sim_id) %>%
   ungroup

}) %>%
  filter(complete.cases(.))


worst_case <- completion_time %>% #
  group_by(alpha, graph) %>%
  summarise(max = max(as.numeric(hours)),
            min = min(as.numeric(hours)))

#worst case 1 simulation from each level
worst_case %>%
  group_by(graph) %>%
  summarise(max_hours = sum(max)) %>%
  arrange(max_hours) %>% #1 simulations from each level
  mutate(
    hours_per_fract_v = max_hours*100,
    hours_set = hours_per_fract_v*10*4, #ten variations per fraction and four fractions 
    hours_all_sets = hours_set*30) %>%
  arrange(max_hours)  
#1 cpu 1 hour clock time per set of twelve


completion_time %>% 
 # filter(mins<3) %>%
  mutate(alpha = factor(alpha),
         mins = as.numeric(mins)) %>%
  ggplot(aes(x = graph, y = mins, fill = alpha)) + geom_boxplot() +
  labs(title = "time to completion by alpha, there are 400 repetitions of each alpha",
       y = "minutes") 

completion_time %>% 
  #filter(graph!="IEEE_14") %>%
  mutate(alpha = factor(alpha),
         mins = as.numeric(mins)) %>%
  ggplot(aes(x  = mins, group = alpha, colour = as.numeric(alpha))) + geom_density()+
  facet_wrap(~graph) + 
  scale_color_viridis_c()

mean_time_per_alpha <- completion_time %>%
  group_by(alpha, graph) %>%
  summarise(mean = mean(as.numeric(mins)),
            median = median(as.numeric(mins)))

mean_time_per_alpha %>%
  group_by(graph) %>%
  summarise(sum_mean = sum(mean),
            sum_median = sum(median)) %>%
  arrange(sum_mean)

set_times <- completion_time %>%
  group_by(graph) %>%
  summarise(hours_fract_v = sum(hours, na.rm = T)) %>% #per fractions so 100 simulations from each level
  mutate(hours_fract_v_round = round(hours_fract_v+0.5, 1),
    hours_set = hours_fract_v_round*10*4, #ten variations per fraction and four fractions 
         hours_all_sets = hours_set*30) %>%
  arrange(hours_fract_v)

set_times

set_times %>%
  mutate(hours_set = hours_set,
         hours_all_sets = hours_all_sets)

test <- completion_time %>%
  group_by(alpha, graph) %>%
  sample_n(size = 10000, replace = T) %>%
  mutate(ID = 1:n()) %>%
  group_by(graph, ID) %>%
  summarise(mins = sum(as.numeric(mins))) %>%
  group_by(graph) %>%
  mutate(percentile = percent_rank(mins)) %>%
  ungroup

test_3 <- test %>%
  filter(percentile>0.99) %>%
  group_by(graph) %>%
  summarise(mins_99th = min(mins)) %>%
  left_join(worst_case %>%
  group_by(graph) %>%
  summarise(max_mins = sum(max)) %>%
  arrange(max_mins) %>%
  mutate()) %>%
  arrange(max_mins)

test %>%
  ggplot(aes(x = mins, colour = graph)) + geom_density()

```

#Run HPC script in parallel

This code tests the HPC script by running it in parallel.Two parallel methods are used foreach and parralell's mcapply. Neither of these methods seem particularly good and it could be an isse related to ram access or something.

This SO question and answer are interesting and related. It may be that ensuring NULL is returned speeds up the process, but I need to check that
https://stackoverflow.com/questions/41925706/why-does-foreach-dopar-get-slower-with-each-additional-node?noredirect=1&lq=1

The chunk uses the parameter file for the HPC and the HPC script to run the data generation for the small graphs

##Run collapse script
```{r}

start_up_file_name <- "attack_base_IEEE_14_igraph.txt"

#Load the params file. 
HPC_params <- read_delim(file.path(HPC_startup_parameter_file_path, start_up_file_name ), delim = " ")

#This appears to be considerably faster than the parallel version
1:nrow(HPC_params) %>% walk(~{
  
  task_id <- .x #this is the compute group to be used
  
  HPC_start_up_file <- start_up_file_name
  
  source(file.path("/home/jonno/Spring_Embeddings_Paper", "HPC_Spring_Embeddings_attack_script.R"), local = TRUE)
  
  return(NULL)
  
})


#do just a single sim from everything
list.files(HPC_startup_parameter_file_path, pattern ="attack") %>% walk(~{
  
  start_up_file_name <- .x
  HPC_params <- read_delim(file.path(HPC_startup_parameter_file_path, .x), delim = " ")
  
  task_id <- 1 #this is the compute group to be used
  
  HPC_start_up_file <- start_up_file_name
  
  source(file.path("/home/jonno/Spring_Embeddings_Paper", "HPC_Spring_Embeddings_attack_script.R"), local = TRUE)
  
  return(NULL)
  
})


#do everything #this probably takes ages
list.files(HPC_startup_parameter_file_path, pattern ="attack") %>% walk(~{
  
  start_up_file_name <- .x
  
  HPC_params <- read_delim(file.path(HPC_startup_parameter_file_path, .x), delim = " ")
  
  1:nrow(HPC_params) %>% walk(~{

    task_id <- .x
    #compute_group <- temp$compute_group #as the df variable is called compute group, this cannot be otherise the variable just has to equal itself and the grouping is not used
    
    HPC_start_up_file <-  start_up_file_name 
    
    source(file.path("/home/jonno/Spring_Embeddings_Paper", "HPC_Spring_Embeddings_attack_script.R"), local = TRUE)
    
    return(NULL)
    
  })
  
  return(NULL)
  
})

```

##Run Strain script
```{r}
#If this is put in a loop or walk mapping then it will "arguably" calculate the data for all the maps and systems
#Load the params file. 


start_up_file_name <- "strain_base_IEEE_14_igraph.txt"

#Load the params file. 
HPC_params <- read_delim(file.path(HPC_startup_parameter_file_path, start_up_file_name ), delim = " ")

#This appears to be considerably faster than the parallel version
1:nrow(HPC_params) %>% walk(~{
  
  task_id <- .x #this is the compute group to be used
  
  HPC_start_up_file <- start_up_file_name

  source(file.path("/home/jonno/Spring_Embeddings_Paper", "HPC_strain_script.R"), local = TRUE)
  
  return(NULL)
  
} )


```

#Load HPC output

submit job to myriad
qsub ~/Spring_Embeddings_Paper/HPC_attack_bash 


##A for loop to submit all the permutation jobs
#!/bin/bash
for i in {2..30}
do
   qsub  ~/Spring_Embeddings_Paper/bash_scripts/HPC_attack_bash_118_permutation $i
done

to send files to myriad
 scp /home/jonno/Useful_PhD__R_Functions.tar.xz ucabbou@myriad.rc.ucl.ac.uk:~


#Extract the data from the zip files

```{r}





list.files("/home/jonno/Dropbox/IEEE_Networks/strain") %>% walk(~{
  
  untar_myriad_strain_files("/home/jonno/Dropbox/IEEE_Networks/strain", .x, extraction_directory = file.path(project_folder, "test_strain"))
  
})


tar_path <-"/home/jonno/HPC_jobs"
  
tar_file <- "files_from_job_2059520.30.tgz"



untar_myriad_collapse_summaries(tar_path, tar_file, extraction_directory = file.path(project_folder, "test_collapse"))

list.files("/home/jonno/HPC_jobs") %>% walk(~{
  
  untar_myriad_collapse_summaries("/home/jonno/HPC_jobs", tar_file = .x, extraction_directory = file.path(project_folder, "test_collapse"))
  
})


```

#First HPC analysis

This code was used in the first HPC analysis that has now been supplanted with the robin hood redistribution
This chunk is to test the output of the model and make sure it all makes sense
```{r}

collapse_point <- c(14,30,57, 118, 300) %>%
  map_df(~{
print(.x)    
    #Load all the collapse files
    collapse_data <- list.files(file.path("/home/jonno/Dropbox/IEEE_Networks/test_collapse", paste0("IEEE_" , .x, "_igraph")), 
                                full.names = TRUE, recursive = TRUE) %>%
      map_df(~read_rds(.x))
    
    #Filter to keep the mean point of GC collapse per collapse setting
    collapse_point <- collapse_data %>%
      group_by(ec, v, fract, permutation) %>%
      mutate(alpha = first(mean_alpha),
             loading = first(mean_loading),
             test = first(edges)) %>%
      filter(mean_degree_sqrd< 2*mean_degree) %>%#the giant component is lost
      arrange(attack_round) %>%
      group_by(ec, v, fract, permutation, simulation_id) %>% #remove all the attacks after the first point at which the giant component is lost
      summarise_all(first) %>%
      ungroup %>%
      group_by(ec, v, fract, permutation) %>%
      mutate(counts = n()) %>%
      summarise_all(mean) %>% ungroup %>%
      mutate(graph = .x)
    
    return(collapse_point)
  })


test_embeddings <- c(14,30,57, 118, 300)  %>%
  map_df(~{
    print(.x)
    #Read the embeddings files
    embeddings_data <- list.files(file.path("/home/jonno/Dropbox/IEEE_Networks/test_strain", paste0("IEEE_" , .x, "_igraph")), 
                                  full.names = TRUE, recursive = TRUE) %>%
      map_df(~{
        
        specs <- basename(.x) %>% gsub(".rds", "", .) %>% 
          str_split(., "_", simplify = TRUE) %>% t(.) %>% 
          as_tibble(., name_repair = "unique") %>%
          pull(V1)
        
        embeddings_list <- read_rds(.x)
        
        temp_strain <- embeddings_list$edge_embeddings %>%
          mutate(tension = k*(H-1)) %>% #there was an error when strain was calculated previously it was simply H*k
          select(strain, tension, alpha, line_load) %>%
          summarise_all(.funs = c(mean = mean, median = median))
        
        temp_elevation <- embeddings_list$node_embeddings %>%
          select(elevation = z, force) %>%
          summarise_all(.funs = c(mean = mean, median = median)) %>%
          select(elevation_median, elevation_mean)
        
        bind_cols(temp_strain, temp_elevation) %>%
          mutate(fract = specs[2],
                 ec = specs[4],
                 v = specs[6])
        
      }) %>%
      mutate(
        ec = as.numeric(ec),
        v = as.numeric(v),
        fract = as.numeric(fract),
        graph = .x)
    
    return(embeddings_data)
    
  })


edges_per_graph <- list.files(power_grid_graphs_path, pattern = ".rds", full.names = TRUE) %>%
  map_df(~{
    
    g <- read_rds(.x)
    
    tibble(graph = str_extract(.x, "([0-9]+)") %>% as.numeric(), edges = ecount(g))
    
  }) 


params <- list.files(analysis_parameter_file_path, pattern = "^base_IEEE", full.names = T) %>%
  map_df(~{read_rds(.x)}) %>%
  filter(simulation_id ==1) %>%
  mutate(graph = str_extract(permutation, "[0-9]+") %>% as.numeric()) %>%
  select(seed:fract, graph)



test_out <- left_join(collapse_point, test_embeddings ) %>%
  select(attack_round,  alpha_a = alpha, strain_mean:line_load_median, v, ec, graph, fract) %>% #line_load_mean = loading,
  filter(complete.cases(.)) %>%
  left_join(., edges_per_graph) %>%
 left_join(params) %>%
  mutate(line_load_mean2 = 1/alpha)
%>%
  filter(graph ==118,
         ec == 2)


#Plot the data
test_out  %>%
  #filter(graph == 57) %>%
    mutate(alpha = 1/alpha) %>%
  pivot_longer(., cols =c("line_load_median", "strain_median"), names_to = "metric" ) %>%
  group_by(metric, graph) %>%
  mutate(value = (value-min(value))/(max(value)-min(value)),
         attack_fract = attack_round/edges) %>%  #rough way of getting the limits
  ggplot(aes(x = value, y = attack_fract)) +
 # scale_color_viridis_d()+
  geom_point(aes(colour = as.factor(ec))) +
 # facet_grid(graph~metric) +
  facet_grid(metric~graph) +
  geom_smooth(method = "loess", formula = y ~ x, se = FALSE)

test_out %>% ggplot(aes(x = line_load_mean, y= strain_mean, colour = factor(ec))) + 
  geom_point() + 
#  scale_color_viridis_c() +
  facet_grid(~graph)



  
  library(yardstick)

#model the fit of strain vs alpha
model_res <- c(14,30,57, 118, 300) %>%
  map_df(~{

    temp <- test_out %>%
      filter(graph==.x) 
    
loess_alpha <- loess(formula = attack_round~ line_load_mean, 
      data = temp)


loess_strain <-  loess(formula = attack_round~ strain_median, 
      data = temp)


model_comp <- temp  %>%
  mutate(alpha_preds = predict(loess_alpha),
         strain_preds = predict(loess_strain))

Out <- bind_rows(
  #the raw relationship
  metrics(temp, attack_round, line_load_median) %>% mutate(type = "alpha",
                                                             class = "raw"),
  metrics(temp, attack_round, strain_mean) %>% mutate(type = "strain",
                                                          class = "raw"),
  #the predictive model
  mape(model_comp, attack_round, alpha_preds) %>% mutate(type = "alpha",
                                                            class = "loess"),
  mape(model_comp, attack_round, strain_preds) %>% mutate(type = "strain",
                                                             class = "loess")
  ) %>%
    mutate(graph = .x) %>%
  rename(metric = .metric, estimate = .estimate) %>%
  select(-.estimator)
  
  return(Out)
})

model_res2 <- model_res %>%
  pivot_wider(., names_from = type, values_from = estimate )


model_res2 %>%
  filter(#metric == "rmse",
         class !="raw") %>%
  arrange(metric)

```


